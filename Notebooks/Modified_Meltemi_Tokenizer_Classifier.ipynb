{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8pfo5o0yCo_"
   },
   "source": [
    "# Building the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUAvwJrhyCo_"
   },
   "source": [
    "## 1. Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1748244599537,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "ge3QBX9V0rhS"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20383,
     "status": "ok",
     "timestamp": 1748244619935,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "I6BcusMEA30l",
    "outputId": "1f637407-1a46-477f-cf6f-b3ed1a5d5676"
   },
   "outputs": [],
   "source": [
    "sys.path.append('drive/MyDrive/')\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5703,
     "status": "ok",
     "timestamp": 1748244625639,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "Ks-zG5BdyCpA",
    "outputId": "9256a108-6667-4b52-e942-4fbb09164528"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "cg_sents = []\n",
    "smg_sents = []\n",
    "\n",
    "def remove_duplicate_punctuation(s):\n",
    "    return re.sub(r'([\\.\\?!;])\\1+', r'\\1 ', s)\n",
    "\n",
    "def fix_new_sentence_spacing(s):\n",
    "    return re.sub(r'([a-zα-ωίϊΐόάέύϋΰήώ])([\\.\\?!;])([A-ZΑ-ΩΆΈΊΌΎΏΉ])', r'\\1\\2 \\3', s)\n",
    "\n",
    "with open('/content/drive/MyDrive/paramithia.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    text = fix_new_sentence_spacing(text)\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        smg_sents += sent_tokenize(line)\n",
    "\n",
    "file = pd.read_csv(\"/content/drive/MyDrive/stories-train.csv\")\n",
    "for text in file['text']:\n",
    "    text = remove_duplicate_punctuation(text)\n",
    "    text = fix_new_sentence_spacing(text)\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        cg_sents += sent_tokenize(line)\n",
    "\n",
    "#with open('./Data/cg_fb.txt', 'r', encoding='utf-8') as in_file:\n",
    " #   text = remove_duplicate_punctuation(in_file.read())\n",
    "  #  text = fix_new_sentence_spacing(text)\n",
    "   # lines = [p for p in text.split('\\n') if p]\n",
    "    #for line in lines:\n",
    "     #   cg_sents += sent_tokenize(line)\n",
    "\n",
    "#with open('./Data/cg_other.txt', 'r', encoding='utf-8') as in_file:\n",
    " #   text = remove_duplicate_punctuation(in_file.read())\n",
    "  #  text = fix_new_sentence_spacing(text)\n",
    "   # lines = [p for p in text.split('\\n') if p]\n",
    "    #for line in lines:\n",
    "     #   cg_sents += sent_tokenize(line)\n",
    "\n",
    "#with open('./Data/smg_twitter.txt', 'r', encoding='utf-8') as in_file:\n",
    " #   text = remove_duplicate_punctuation(in_file.read())\n",
    "  #  text = fix_new_sentence_spacing(text)\n",
    "   # lines = [p for p in text.split('\\n') if p]\n",
    "    #for line in lines:\n",
    "     #   smg_sents += sent_tokenize(line)\n",
    "\n",
    "#with open('./Data/smg_fb.txt', 'r', encoding='utf-8') as in_file:\n",
    " #   text = remove_duplicate_punctuation(in_file.read())\n",
    "  #  text = fix_new_sentence_spacing(text)\n",
    "   # lines = [p for p in text.split('\\n') if p]\n",
    "    #for line in lines:\n",
    "     #   smg_sents += sent_tokenize(line)\n",
    "\n",
    "#with open('./Data/smg_other.txt', 'r', encoding='utf-8') as in_file:\n",
    "   # text = remove_duplicate_punctuation(in_file.read())\n",
    "    #text = fix_new_sentence_spacing(text)\n",
    "    #lines = [p for p in text.split('\\n') if p]\n",
    "    #for line in lines:\n",
    "     #   smg_sents += sent_tokenize(line)\n",
    "\n",
    "cg_sents = cg_sents[:1988]\n",
    "print(len(cg_sents))\n",
    "print(len(smg_sents))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ206Yn9yCpB"
   },
   "source": [
    "## 2. Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353,
     "referenced_widgets": [
      "ec709d86c33c44d78dc63b4e5658787a",
      "2ddfd7d8727941f28090faf14cd12f22",
      "e8e7dd4fdeef4eb89a55bf0491e2835c",
      "8d30e5e0b8914651ba6aeb4c699bef7c",
      "da89ec2cb60e4af7bc2039ae6a126379",
      "63d58bf2e0834191a32f21d7d55383d3",
      "38e5cff026924eaeb874fcca42074745",
      "984dc96a2bf94af0930cdb8be511d116",
      "1375292a820849bc902c32ceceeeebd8",
      "8342a9a5531b4b2f88f44d06e4fbff8f",
      "bd4b6f6b0c7545988b8959cea1592132",
      "f45e2531205a4ecaaa6e03c2bf733b9b",
      "81ab6737cc484345b1847a87864ba77b",
      "c236ee4b21584f3f86c30ed46ddd3bcc",
      "ac35ffeda2974f4eadcf69c3ca8d41c8",
      "f3a2839519764a8fbb3bf21c896153b3",
      "dd5c0cdaf07447cd97e3e553b3fb2fb7",
      "7fa64b38a7fb4ef19cfd089a556f0137",
      "20f7b92be52847f89e408565d54ddbaa",
      "18d04f71905d4ee4a0cf457a5d7b09fd",
      "a8250c63c00445359d37a725ac80bcbf",
      "458eb5d1e47e4268bc2ff5b552079df5",
      "85398b20e23047cb945f7170025691af",
      "ad69b06f0e16474b8a0782143aa11c7d",
      "09f794013acf4a5fbd09d734b72d7821",
      "88e985c8f6104f6e8a46a15d50c976bf",
      "71239df6d23c40488f930b562e802a19",
      "71bc0c4042044924ba1248564069a9e3",
      "5412fdbb422b4c1b92333da4cf7a7299",
      "91edf01f64ce4c9aaa78a887747068f3",
      "95bf94ac842e478e9527a1e518562a03",
      "875e7d68071743e1aad77e1718520792",
      "df2e49b719de4796b16c3f9220f6c70a",
      "dc060c706c494560be5d67abb9f0592d",
      "7ee1c432ac3f40db8e792b91eaa0e380",
      "b45d20bcf87b482ea9611f4b96b3c2ff",
      "468f8cf856b540d1846c62c8251c2ad7",
      "8ed73200a6104bbe814b614aa21ef283",
      "e2410c030479412998a09da28a7d9599",
      "739b54cfad674bb4b03a5b61e70d5668",
      "6410c5dfffe94df9850659ef13c56f41",
      "5d207721d93445b7a7a260a41d07a155",
      "0bd9ca67caac425ebfc8524a79b37b28",
      "eb4a3504398f43a7a0bb5464d5047122",
      "544b3768222f464cb6a732db78dc5393",
      "bab3612d304042c78b3a5558fe2117f7",
      "9cfabbbcf8684aec8c3b14530c83cdda",
      "b8e5e5d710cc455cae4a52556f6253c9",
      "01065c8a97d144479f2265491ebb7ef3",
      "1047838294c14644af524269f26cfb43",
      "a3192ecd8130439bb7152d70dad2c8c2",
      "7ad046f3920740d78544a684d2fb03e5",
      "f33ae23b5fdd4127b9d5f75cf9c94a43",
      "62d9dd92de614bb18b0abf147ed7baab",
      "07129b08665642d7b004d1d1248683a0"
     ]
    },
    "executionInfo": {
     "elapsed": 16406,
     "status": "ok",
     "timestamp": 1748244642053,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "UmEiZ7gTyCpB",
    "outputId": "3a03505b-0c79-44a0-f88b-785e892f3b25"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from string import punctuation\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "meltemi_tokenizer = AutoTokenizer.from_pretrained(\"ilsp/Meltemi-7B-Instruct-v1.5\")\n",
    "\n",
    "punctuation += '´΄’…“”–—―»«'\n",
    "\n",
    "def contains_english(sentence):\n",
    "    return re.search(r'[a-zA-Z]', sentence) is not None\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def get_clean_sent_el(sentence):\n",
    "    if contains_english(sentence):\n",
    "        return ''\n",
    "    sentence = re.sub(r'\\d+', '', sentence)\n",
    "    sentence = re.sub(r'^RT', '', sentence)\n",
    "    sentence = re.sub(r'\\&\\w*;', '', sentence)\n",
    "    sentence = re.sub(r'\\@\\w*', '', sentence)\n",
    "    sentence = re.sub(r'\\$\\w*', '', sentence)\n",
    "    sentence = re.sub(r'https?:\\/\\/.*\\/\\w*', '', sentence)\n",
    "    sentence = ''.join(c for c in sentence if c <= '\\uFFFF')\n",
    "    sentence = strip_accents(sentence)\n",
    "    sentence = re.sub(r'#\\w*', '', sentence)\n",
    "    sentence = sentence.lower()\n",
    "    tokens = WhitespaceTokenizer().tokenize(sentence)\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token == 'ο,τι' or token == 'ό,τι' or token == 'o,ti' or token == 'ó,ti':\n",
    "            new_tokens.append(token)\n",
    "        else:\n",
    "            token = re.sub(r'(?<=[.,!\\?;\\'΄´])(?=[^\\s])', r' ', token)\n",
    "            new_token = token.translate(str.maketrans({key: None for key in punctuation}))\n",
    "            if new_token != '':\n",
    "                new_tokens.append(new_token)\n",
    "    sentence =' '.join(new_tokens)\n",
    "    sentence = sentence.replace('\\ufeff', '')\n",
    "    sentence = sentence.strip(' ')\n",
    "    sentence = sentence.replace('  ', ' ')\n",
    "    return sentence\n",
    "\n",
    "cg_sents_clean = []\n",
    "smg_sents_clean = []\n",
    "\n",
    "for sent in cg_sents:\n",
    "    cg_sents_clean.append(get_clean_sent_el(sent))\n",
    "for sent in smg_sents:\n",
    "    smg_sents_clean.append(get_clean_sent_el(sent))\n",
    "\n",
    "cg_sents_clean = list(filter(None, cg_sents_clean))\n",
    "smg_sents_clean = list(filter(None, smg_sents_clean))\n",
    "cg_sents_clean[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1748244642070,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "9785dcb8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_meltemi_tokens(texts):\n",
    "    token_lists = []\n",
    "    for text in texts:\n",
    "        encoded = meltemi_tokenizer(text, add_special_tokens=False)\n",
    "        tokens = encoded.tokens()\n",
    "        token_lists.append(tokens)\n",
    "    return token_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1748244642368,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "22e6fc4c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Apply Meltemi tokenizer to Cypriot and Standard Greek cleaned sentences\n",
    "cg_token_lists = get_meltemi_tokens(cg_sents_clean)\n",
    "smg_token_lists = get_meltemi_tokens(smg_sents_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1748244642387,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "380845ad"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert token lists back into space-separated strings for vectorizer\n",
    "cg_texts_tokenized = [' '.join(tokens) for tokens in cg_token_lists]\n",
    "smg_texts_tokenized = [' '.join(tokens) for tokens in smg_token_lists]\n",
    "\n",
    "# Combine for classification\n",
    "all_texts = cg_texts_tokenized + smg_texts_tokenized\n",
    "labels = [1]*len(cg_texts_tokenized) + [0]*len(smg_texts_tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl5GSG6byCpB"
   },
   "source": [
    "## 3. Building the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1748244642409,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "hV0TxT5ByCpC"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def get_word_ngrams(tokens, n):\n",
    "    ngrams_list = []\n",
    "    ngrams_list.append(list(ngrams(tokens, n)))\n",
    "    ngrams_flat_tuples = [ngram for ngram_list in ngrams_list for ngram in ngram_list]\n",
    "    format_string = '%s'\n",
    "    for _ in range(1, n):\n",
    "        format_string += (' %s')\n",
    "    ngrams_list_flat = [format_string % ngram_tuple for ngram_tuple in ngrams_flat_tuples]\n",
    "    return ngrams_list_flat\n",
    "\n",
    "def get_char_ngrams(word, n):\n",
    "    ngrams_list = []\n",
    "    word = word.replace('ς', 'σ')\n",
    "    ngrams_list.append(list(ngrams(word, n, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')))\n",
    "\n",
    "    # Removing redundant ngrams:\n",
    "    if n > 2:\n",
    "        redundant_combinations = n - 2\n",
    "        ngrams_list = [ngram_list[redundant_combinations : -redundant_combinations] for ngram_list in ngrams_list]\n",
    "\n",
    "    ngrams_flat_tuples = [ngram for ngram_list in ngrams_list for ngram in ngram_list]\n",
    "    format_string = ''\n",
    "    for _ in range(0, n):\n",
    "        format_string += ('%s')\n",
    "    ngrams_list_flat = [format_string % ngram_tuple for ngram_tuple in ngrams_flat_tuples]\n",
    "    return ngrams_list_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1748244642454,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "JFZm_Lj5yCpC",
    "outputId": "ae2a7344-16c4-40d2-8258-7291f7e51618"
   },
   "outputs": [],
   "source": [
    "# Feature extractor\n",
    "def get_ngram_features(sent): # The reason I do not use NLTK's everygrams to extract the features quickly is because the behavior of my n-gram extractor is modified to remove redundant n-grams. Also, I need to label word and char n-grams to avoid ambiguity\n",
    "    sentence_tokens = WhitespaceTokenizer().tokenize(sent)\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Word unigrams\n",
    "    ngrams = get_word_ngrams(sentence_tokens, 1)\n",
    "    for ngram in ngrams:\n",
    "        features[f'word({ngram})'] = features.get(f'word({ngram})', 0) + 1 # The second parameter to .get() is a default value if the key doesn't exist.\n",
    "\n",
    "    # Word bigrams\n",
    "    ngrams = get_word_ngrams(sentence_tokens, 2)\n",
    "    for ngram in ngrams:\n",
    "        features[f'word_bigram({ngram})'] = features.get(f'word_bigram({ngram})', 0) + 1\n",
    "\n",
    "    # Char unigrams\n",
    "    #for word in sentence_tokens:\n",
    "     #   ngrams = get_char_ngrams(word, 1)\n",
    "      #  for ngram in ngrams:\n",
    "       #     features[f'char({ngram})'] = features.get(f'char({ngram})', 0) + 1\n",
    "\n",
    "    # Char bigrams\n",
    "    #for word in sentence_tokens:\n",
    "     #   ngrams = get_char_ngrams(word, 2)\n",
    "      #  for ngram in ngrams:\n",
    "       #     features[f'char_bigram({ngram})'] = features.get(f'char_bigram({ngram})', 0) + 1\n",
    "\n",
    "    # Char trigrams\n",
    "   # for word in sentence_tokens:\n",
    "    #    ngrams = get_char_ngrams(word, 3)\n",
    "     #   for ngram in ngrams:\n",
    "      #      features[f'char_trigram({ngram})'] = features.get(f'char_trigram({ngram})', 0) + 1\n",
    "\n",
    "    return features\n",
    "\n",
    "get_ngram_features('αυτη ειναι η σπαρτη')\n",
    "get_ngram_features(cg_texts_tokenized[0])\n",
    "get_ngram_features(smg_texts_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1748244642482,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "lJEbrtGIyCpD"
   },
   "outputs": [],
   "source": [
    "# from nltk import everygrams\n",
    "\n",
    "# def sent_process(sent):\n",
    "#     return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4)\n",
    "#             if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n",
    "\n",
    "# sent_process('αυτη ειναι η σπαρτη')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9D28HhKyCpD"
   },
   "source": [
    "## 4. Creating the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1748244642809,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "sz8_fyjoyCpD",
    "outputId": "9e71bbe2-4ad2-4ab3-9f08-f176e184fd6f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_sents_labeled = ([(sentence, 'CG') for sentence in cg_texts_tokenized] + [(sentence, 'SMG') for sentence in smg_texts_tokenized])  ##\n",
    "random.shuffle(all_sents_labeled)\n",
    "all_sents_labeled[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1748244642821,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "Wf9Kmz49yCpD",
    "outputId": "7ee65fcd-8178-496c-ee59-36fd053c10f3"
   },
   "outputs": [],
   "source": [
    "NO_ALL_SENTENCES = len(all_sents_labeled)\n",
    "NO_TRAIN_SENTENCES = round(NO_ALL_SENTENCES * .8)\n",
    "\n",
    "train_set = all_sents_labeled[:NO_TRAIN_SENTENCES]\n",
    "test_set = all_sents_labeled[NO_TRAIN_SENTENCES:]\n",
    "\n",
    "train_set_sents = [sent[0] for sent in train_set]\n",
    "train_set_labels = [sent[1] for sent in train_set]\n",
    "test_set_sents = [sent[0] for sent in test_set]\n",
    "test_set_labels = [sent[1] for sent in test_set]\n",
    "\n",
    "print(train_set_sents[0], train_set_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1748244642840,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "PzAq05zMyCpD",
    "outputId": "96c39739-0182-4382-d3f1-eb0c957ab235"
   },
   "outputs": [],
   "source": [
    "print('DATASET\\t', 'SENTENCES')\n",
    "print('All\\t', NO_ALL_SENTENCES)\n",
    "print('Training', NO_TRAIN_SENTENCES)\n",
    "print('Testing\\t', NO_ALL_SENTENCES - NO_TRAIN_SENTENCES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iaCgKiWyCpE"
   },
   "source": [
    "## 5. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1748244642853,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "guC2oturyCpE",
    "outputId": "ad094888-fa7e-4b1f-ac86-d6d122007edf"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=get_ngram_features)\n",
    "\n",
    "train_set_vectors = count_vect.fit_transform(train_set_sents)\n",
    "test_set_vectors = count_vect.transform(test_set_sents) # Unlike fit_transform(), transform() does not change the count vectorizer's vocabulary so it should be used for the test set.\n",
    "train_set_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1748244643166,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "U_x75V-UyCpE",
    "outputId": "ffc0db39-cff6-4006-c61a-3abe9b6c84a7"
   },
   "outputs": [],
   "source": [
    "from numpy import set_printoptions, nan\n",
    "set_printoptions(threshold=sys.maxsize) # Prints whole array. Required because by default an array with thousands of elements wouldn't be printed in full.\n",
    "\n",
    "train_set_vectors.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1748244643324,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "yDJuwZJfyCpE",
    "outputId": "88d75a4f-c81b-4381-dde1-82dd76808d62"
   },
   "outputs": [],
   "source": [
    "count_vect.vocabulary_ # The numbers are not counts but indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1748244643353,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "4rWqjQxcyCpE",
    "outputId": "05aae544-519b-4fb6-faa4-aeb843967d2c"
   },
   "outputs": [],
   "source": [
    "len(count_vect.vocabulary_) # This is the same as the length of each vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWka6RrbyCpF"
   },
   "source": [
    "## 6. Building the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1748244723722,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "_feA9Sm8yCpF"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def show_performance_header_and_accuracy(predictions):\n",
    "    print('\\t\\t\\tPERFORMANCE\\n')\n",
    "    print('Accuracy:', round(accuracy_score(test_set_labels, predictions), 2), '\\n')\n",
    "\n",
    "def show_confusion_matrix(cm):\n",
    "    print('\\t         Predicted')\n",
    "    print('\\t        CG       SMG')\n",
    "    print('\\t     -------- --------')\n",
    "    print('\\tCG  | {:^6} | {:^6}'.format(cm[0][0], cm[0][1]))\n",
    "    print('Actual\\t     -------- --------')\n",
    "    print('\\tSMG | {:^6} | {:^6}'.format(cm[1][0], cm[1][1]))\n",
    "\n",
    "def show_most_informative_features(vectorizer, clf, n=10):\n",
    "    print(\"\\t\\t    SMG\\t\\t\\t\\t\\t\\t    CG\\n\")\n",
    "    feature_names = vectorizer.get_feature_names_out()                             ##\n",
    "    coefs_with_fns = sorted(zip(clf.feature_log_prob_[0], feature_names))          ##\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%17s\\t\\t\\t%.4f\\t%17s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJfKTd09yCpF"
   },
   "source": [
    "### 6.1 Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1748244725678,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "W5mI49AXyCpF",
    "outputId": "ddc4d196-e08c-4420-e1ae-9b5ba9bb492e"
   },
   "outputs": [],
   "source": [
    "clf_multinomialNB = MultinomialNB() # There are no params for MultinomialDB that prevent overfitting, so any overfitting is caused by the small dataset size.\n",
    "clf_multinomialNB.fit(train_set_vectors, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1748244726610,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "Iy5R69NbyCpF",
    "outputId": "d3cd22a8-6e7d-42d0-fc02-abc974c9b065"
   },
   "outputs": [],
   "source": [
    "clf_multinomialNB_predictions = clf_multinomialNB.predict(test_set_vectors)\n",
    "\n",
    "show_performance_header_and_accuracy(clf_multinomialNB_predictions)\n",
    "\n",
    "print(classification_report(test_set_labels, clf_multinomialNB_predictions))\n",
    "\n",
    "cmatrix = confusion_matrix(test_set_labels, clf_multinomialNB_predictions, labels=[\"CG\", \"SMG\"])    ##\n",
    "show_confusion_matrix(cmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748244729241,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "htQVeylpyCpF",
    "outputId": "82110675-dde0-4b49-9f22-482434a4789b"
   },
   "outputs": [],
   "source": [
    "show_most_informative_features(count_vect, clf_multinomialNB, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_S2VqU2IyCpF"
   },
   "source": [
    "### 6.2 Linear Support Vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1748244643396,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "PP8nC6FayCpF",
    "outputId": "01b716fc-5d21-4d6c-f034-1a84a62c2cc5"
   },
   "outputs": [],
   "source": [
    "clf_linearSVC = LinearSVC(max_iter=1500) # n_samples < n_features in training set so the dual param is kept at its default value of True. Default max_iter = 1000\n",
    "clf_linearSVC.fit(train_set_vectors, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1748244643403,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "Q98EgS3GyCpG",
    "outputId": "92164fc2-9b7c-4310-9445-128879576b1b"
   },
   "outputs": [],
   "source": [
    "clf_linearSVC_predictions = clf_linearSVC.predict(test_set_vectors)\n",
    "\n",
    "show_performance_header_and_accuracy(clf_linearSVC_predictions)\n",
    "\n",
    "print(classification_report(test_set_labels, clf_linearSVC_predictions))\n",
    "\n",
    "cmatrix = confusion_matrix(test_set_labels, clf_linearSVC_predictions)\n",
    "show_confusion_matrix(cmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "error",
     "timestamp": 1748244643414,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "iXz4h4jVyCpG",
    "outputId": "786934e0-b944-4569-b8be-e50158fd5870"
   },
   "outputs": [],
   "source": [
    "show_most_informative_features(count_vect, clf_linearSVC, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxkklKpcyCpG"
   },
   "source": [
    "### 6.3 Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1748244643458,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "YgXsPF3fyCpG"
   },
   "outputs": [],
   "source": [
    "clf_logisticRegression = LogisticRegression() # Again, dual = True. Default solver = 'liblinear'. It's recommended for smaller databases. For bigger databases, 'saga' could be used.\n",
    "clf_logisticRegression.fit(train_set_vectors, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44263,
     "status": "aborted",
     "timestamp": 1748244643461,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "WC75yWMFyCpG"
   },
   "outputs": [],
   "source": [
    "clf_logisticRegression_predictions = clf_logisticRegression.predict(test_set_vectors)\n",
    "\n",
    "show_performance_header_and_accuracy(clf_logisticRegression_predictions)\n",
    "\n",
    "print(classification_report(test_set_labels, clf_logisticRegression_predictions))\n",
    "\n",
    "cmatrix = confusion_matrix(test_set_labels, clf_logisticRegression_predictions)\n",
    "show_confusion_matrix(cmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44277,
     "status": "aborted",
     "timestamp": 1748244643475,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "U39KxnjuyCpG"
   },
   "outputs": [],
   "source": [
    "show_most_informative_features(count_vect, clf_logisticRegression, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_Fk5gBJyCpG"
   },
   "source": [
    "**It seems that the classification algorithm with the best performance is *Multinomial Naive Bayes***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5XwMhMByCpG"
   },
   "source": [
    "## 7. Analyzing misclassifications made by the Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44279,
     "status": "aborted",
     "timestamp": 1748244643478,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "bH-lqZNdyCpH"
   },
   "outputs": [],
   "source": [
    "print('MISCLASSIFICATIONS\\n')\n",
    "\n",
    "misclassificationCount = 0\n",
    "\n",
    "for i, sent in enumerate(test_set_sents):\n",
    "    if test_set_labels[i] != clf_multinomialNB_predictions[i]:\n",
    "        misclassificationCount += 1\n",
    "        print(f'{misclassificationCount}.', sent, f'(CORRECT = {test_set_labels[i]},', f'PREDICTED = {clf_multinomialNB_predictions[i]})\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3oYFT2gyCpH"
   },
   "source": [
    "## 8. Trying the Multinomial Naive Bayes classifier with custom input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZvUK5dUyCpH"
   },
   "source": [
    "First, a more powerful version of the classifier is built by using all the data available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44281,
     "status": "aborted",
     "timestamp": 1748244643481,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "n9HYdYkZyCpH"
   },
   "outputs": [],
   "source": [
    "full_set_sents = [sent[0] for sent in all_sents_labeled]\n",
    "full_set_labels = [sent[1] for sent in all_sents_labeled]\n",
    "full_set_vectors = count_vect.fit_transform(full_set_sents)\n",
    "\n",
    "clf_super_multinomialNB = MultinomialNB()\n",
    "clf_super_multinomialNB.fit(full_set_vectors, full_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xONQnBpvyCpH"
   },
   "source": [
    "Trying 2 custom sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44283,
     "status": "aborted",
     "timestamp": 1748244643484,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "5GHaAsIHyCpP"
   },
   "outputs": [],
   "source": [
    "cgSent = 'Η Κύπρος εν που τες πιο όμορφες χώρες.'\n",
    "smgSent = 'Η Κύπρος είναι από τις πιο όμορφες χώρες.'\n",
    "\n",
    "demoSentences = [cgSent, smgSent]\n",
    "\n",
    "cgSent = get_clean_sent_el(cgSent)\n",
    "smgSent = get_clean_sent_el(smgSent)\n",
    "\n",
    "test_vec = count_vect.transform([cgSent, smgSent])\n",
    "\n",
    "for sentenceNumber, predictionArr in enumerate(clf_super_multinomialNB.predict_proba(test_vec)):\n",
    "    print(f'SENTENCE {sentenceNumber + 1}: “{demoSentences[sentenceNumber]}”')\n",
    "    if predictionArr[0] > predictionArr[1]:\n",
    "        print(f'PREDICTION: Cypriot Greek (Confidence: {predictionArr[0]:.2f})\\n')\n",
    "    else:\n",
    "        print(f'PREDICTION: Standard Modern Greek (Confidence: {predictionArr[1]:.2f})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44286,
     "status": "aborted",
     "timestamp": 1748244643487,
     "user": {
      "displayName": "Marios Koumides",
      "userId": "00541351652308036986"
     },
     "user_tz": -180
    },
    "id": "e0c87149"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Meltemi tokenizer\n",
    "meltemi_tokenizer = AutoTokenizer.from_pretrained(\"ilsp/Meltemi-7B-Instruct-v1.5\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
